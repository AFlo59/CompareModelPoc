"""
Chatbot optimis√© avec gestion d'erreurs et performance am√©lior√©es
"""

import logging
import sys
import time
from typing import Any, Dict, List, Optional

import streamlit as st

from src.ai.api_client import get_anthropic_client, get_deepseek_client, get_openai_client
from src.ai.models_config import CHAT_DEFAULTS, ModelProvider, calculate_estimated_cost, get_model_config
from src.data.database import get_connection

logger = logging.getLogger(__name__)


class ChatbotError(Exception):
    """Exception personnalis√©e pour les erreurs de chatbot."""

    pass


class APIManager:
    """Gestionnaire optimis√© des appels API avec retry et timeout."""

    @staticmethod
    def call_openai_model(model_config, messages: List[Dict], temperature: float = None) -> Dict[str, Any]:
        """Appelle un mod√®le OpenAI avec gestion d'erreurs."""
        client = get_openai_client()
        temperature = temperature or model_config.temperature_default

        try:
            response = client.chat.completions.create(
                model=model_config.api_name,
                messages=messages,
                temperature=temperature,
                max_tokens=model_config.max_tokens,
                timeout=CHAT_DEFAULTS["timeout"],
            )

            return {
                "content": response.choices[0].message.content,
                "tokens_in": response.usage.prompt_tokens,
                "tokens_out": response.usage.completion_tokens,
                "model": model_config.name,
            }
        except Exception as e:
            error_str = str(e)
            logger.error(f"Erreur OpenAI pour {model_config.name}: {e}")

            # D√©tecter les erreurs de quota sp√©cifiques
            if (
                "billing_hard_limit_reached" in error_str
                or "insufficient_quota" in error_str
                or "You exceeded your current quota" in error_str
                or "Billing hard limit has been reached" in error_str
            ):
                raise ChatbotError(
                    f"Quota OpenAI d√©pass√©: Votre limite de facturation a √©t√© atteinte. V√©rifiez votre compte OpenAI."
                )
            elif "rate limit" in error_str.lower() or "429" in error_str:
                raise ChatbotError(f"Rate limit OpenAI: Trop de requ√™tes. Veuillez patienter quelques secondes.")
            else:
                raise ChatbotError(f"Erreur OpenAI: {str(e)}")

    @staticmethod
    def call_anthropic_model(model_config, messages: List[Dict], temperature: float = None) -> Dict[str, Any]:
        """Appelle un mod√®le Anthropic avec gestion d'erreurs."""
        client = get_anthropic_client()
        temperature = temperature or model_config.temperature_default

        try:
            # S√©parer le message syst√®me des autres messages
            system_msg = ""
            user_messages = []

            for msg in messages:
                if msg["role"] == "system":
                    system_msg = msg["content"]
                else:
                    user_messages.append(msg)

            # S'assurer qu'il y a au moins un message utilisateur
            if not user_messages:
                user_messages = [{"role": "user", "content": "Commen√ßons l'aventure !"}]

            response = client.messages.create(
                model=model_config.api_name,
                max_tokens=model_config.max_tokens,
                temperature=temperature,
                system=system_msg if system_msg else "Tu es un assistant IA.",
                messages=user_messages,
                timeout=CHAT_DEFAULTS["timeout"],
            )

            return {
                "content": response.content[0].text,
                "tokens_in": response.usage.input_tokens,
                "tokens_out": response.usage.output_tokens,
                "model": model_config.name,
            }
        except Exception as e:
            logger.error(f"Erreur Anthropic pour {model_config.name}: {e}")
            raise ChatbotError(f"Erreur Anthropic: {str(e)}")

    @staticmethod
    def call_deepseek_model(model_config, messages: List[Dict], temperature: float = None) -> Dict[str, Any]:
        """Appelle un mod√®le DeepSeek (API compatible OpenAI)."""
        client = get_deepseek_client()
        if client is None:
            raise ChatbotError("Cl√© API DeepSeek manquante")

        temperature = temperature or model_config.temperature_default

        try:
            response = client.chat.completions.create(
                model=model_config.api_name,
                messages=messages,
                temperature=temperature,
                max_tokens=model_config.max_tokens,
                timeout=CHAT_DEFAULTS["timeout"],
            )

            return {
                "content": response.choices[0].message.content,
                "tokens_in": response.usage.prompt_tokens,
                "tokens_out": response.usage.completion_tokens,
                "model": model_config.name,
            }
        except Exception as e:
            logger.error(f"Erreur DeepSeek pour {model_config.name}: {e}")
            raise ChatbotError(f"Erreur DeepSeek: {str(e)}")


def call_ai_model_optimized(model_name: str, messages: List[Dict], temperature: float = None) -> Dict[str, Any]:
    """
    Appelle le mod√®le d'IA appropri√© avec gestion optimis√©e.

    Args:
        model_name: Nom du mod√®le √† utiliser
        messages: Liste des messages de conversation
        temperature: Temp√©rature pour la g√©n√©ration (optionnel)

    Returns:
        Dict contenant 'content', 'tokens_in', 'tokens_out', 'model'

    Raises:
        ChatbotError: En cas d'erreur dans l'appel API
    """
    model_config = get_model_config(model_name)

    try:
        if model_config.provider == ModelProvider.OPENAI.value:
            return APIManager.call_openai_model(model_config, messages, temperature)
        elif model_config.provider == ModelProvider.ANTHROPIC.value:
            return APIManager.call_anthropic_model(model_config, messages, temperature)
        elif model_config.provider == ModelProvider.DEEPSEEK.value:
            return APIManager.call_deepseek_model(model_config, messages, temperature)
        else:
            # Fallback vers GPT-4 pour les mod√®les non support√©s
            logger.warning(f"Mod√®le {model_name} non support√©, fallback vers GPT-4")
            fallback_config = get_model_config("GPT-4")
            return APIManager.call_openai_model(fallback_config, messages, temperature)

    except ChatbotError:
        raise  # Re-raise les erreurs de chatbot
    except Exception as e:
        logger.error(f"Erreur inattendue avec le mod√®le {model_name}: {e}")
        raise ChatbotError(f"Erreur inattendue: {str(e)}")


def store_message_optimized(user_id: int, role: str, content: str, campaign_id: Optional[int] = None) -> None:
    """Stocke un message avec gestion d'erreurs optimis√©e."""
    try:
        with get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute(
                "INSERT INTO messages (user_id, role, content, campaign_id) VALUES (?, ?, ?, ?)",
                (user_id, role, content, campaign_id),
            )
            conn.commit()
    except Exception as e:
        logger.error(f"Erreur stockage message: {e}")
        # Ne pas propager l'erreur pour ne pas casser l'exp√©rience utilisateur
        st.warning("Message non sauvegard√© (erreur technique)")


def store_performance_optimized(
    user_id: int, model: str, latency: float, tokens_in: int, tokens_out: int, campaign_id: Optional[int] = None
) -> None:
    """Stocke les donn√©es de performance avec calcul de co√ªt."""
    try:
        estimated_cost = calculate_estimated_cost(model, tokens_in, tokens_out)

        with get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute(
                """INSERT INTO performance_logs
                   (user_id, model, latency, tokens_in, tokens_out, campaign_id)
                   VALUES (?, ?, ?, ?, ?, ?)""",
                (user_id, model, latency, tokens_in, tokens_out, campaign_id),
            )
            conn.commit()

        logger.info(f"Performance enregistr√©e: {model}, {latency:.2f}s, co√ªt estim√©: ${estimated_cost:.4f}")
    except Exception as e:
        logger.error(f"Erreur stockage performance: {e}")
        # Ne pas propager l'erreur


def launch_chat_interface_optimized(user_id: int) -> None:
    """Interface de chat optimis√©e avec gestion d'erreurs am√©lior√©e."""

    # V√©rification de la campagne
    if "campaign" not in st.session_state or not st.session_state.campaign:
        st.error("‚ùå Aucune campagne s√©lectionn√©e")
        if st.button("üè∞ Choisir une campagne"):
            st.session_state.page = "campaign_or_resume"
            st.rerun()
        return

    campaign = st.session_state.campaign
    campaign_id = campaign.get("id")

    # *** INITIALISATION AUTOMATIQUE POUR NOUVELLE CAMPAGNE ***
    # V√©rifier si c'est une nouvelle campagne (pas d'historique existant)
    # D√©sactiver en mode test pour √©viter les conflits
    is_test_mode = (
        getattr(st.session_state, "_test_mode", False)
        or "pytest" in sys.modules
        or hasattr(st.session_state, "campaign")
        and not hasattr(st.session_state, "user")
    )

    try:
        from src.data.models import get_campaign_messages

        existing_messages = get_campaign_messages(user_id, campaign_id, limit=1)

        if not existing_messages and "history" not in st.session_state and not is_test_mode:
            # Nouvelle campagne : initialiser avec un message de bienvenue
            st.info("üéâ Nouvelle campagne d√©tect√©e ! Initialisation en cours...")

            # Message syst√®me initial
            system_msg = "Tu es un MJ immersif, concis quand n√©cessaire, et tu avances l'histoire sc√®ne par sc√®ne."
            store_message_optimized(user_id, "system", system_msg, campaign_id)

            # Message d'introduction automatique
            campaign_name = campaign.get("name", "Aventure Inconnue")
            campaign_themes = campaign.get("themes", ["Fantasy"])
            intro_prompt = f"Commence une nouvelle aventure dans la campagne '{campaign_name}' avec les th√®mes {', '.join(campaign_themes)}. Pr√©sente l'univers et la situation initiale."

            # Initialiser l'historique avec l'introduction
            st.session_state.history = [{"role": "system", "content": system_msg}, {"role": "user", "content": intro_prompt}]

            # D√©clencher la g√©n√©ration automatique
            st.session_state.auto_start_intro = True
            store_message_optimized(user_id, "user", intro_prompt, campaign_id)

            st.success(f"‚úÖ Campagne '{campaign_name}' initialis√©e !")
            time.sleep(1)  # Petite pause pour l'UX

    except Exception as e:
        logger.error(f"Erreur lors de l'initialisation de la campagne : {e}")

    # D√©terminer le mod√®le automatiquement (campagne -> pr√©f√©rence utilisateur -> d√©faut)
    try:
        from src.data.models import get_user_model_choice

        user_pref = get_user_model_choice(user_id)
    except Exception:
        user_pref = None

    model = campaign.get("ai_model") or user_pref or "GPT-4"
    model_config = get_model_config(model)
    # Afficher une m√©trique informative plut√¥t qu'un s√©lecteur
    info_col1, info_col2 = st.columns([3, 1])
    with info_col1:
        st.caption(f"ü§ñ Mod√®le actif: {model}")
    with info_col2:
        st.metric("üí∞ Co√ªt/1K tokens", f"${model_config.cost_per_1k_input:.3f}")

    # Initialisation de l'historique
    if "history" not in st.session_state:
        st.session_state.history = []

    # D√©marrage automatique si demand√© (auto_start_intro)
    auto_trigger = False
    try:
        auto_trigger = bool(getattr(st.session_state, "auto_start_intro", False) or st.session_state.get("auto_start_intro"))
    except Exception:
        auto_trigger = False

    # Affichage de l'historique AVANT, champ de saisie APR√àS avec am√©liorations visuelles
    for i, msg in enumerate(st.session_state.history):
        with st.chat_message(msg["role"]):
            if msg["role"] == "assistant" and ("‚ùå" in msg["content"] or "Erreur" in msg["content"]):
                # Messages d'erreur avec style sp√©cial
                st.error(msg["content"])
            else:
                st.markdown(msg["content"])

            # Timestamp pour les messages r√©cents (optionnel)
            if i >= len(st.session_state.history) - 3:  # 3 derniers messages
                st.caption(f"Message #{i+1}")

    # Container pour le nouveau message (auto-scroll) - sera utilis√© plus tard
    # message_anchor = st.empty()  # Comment√© car non utilis√© actuellement

    # Champ de saisie (en bas)
    prompt = st.chat_input("Votre action ?")
    user_submitted = bool(prompt)

    if auto_trigger or user_submitted:
        if auto_trigger:
            # Retirer le flag pour ne pas boucler
            try:
                st.session_state.auto_start_intro = False
            except Exception:
                try:
                    st.session_state["auto_start_intro"] = False
                except Exception:
                    pass
            # Si l'historique initial n'a pas √©t√© persist√© (changement de page rapide), persister en t√¢che de fond
            try:
                from src.data.models import get_campaign_messages

                messages = get_campaign_messages(user_id, campaign_id, 2)
                if not messages:
                    # Reconstituer les deux premiers messages si absents
                    store_message_optimized(
                        user_id,
                        "system",
                        "Tu es un MJ immersif, concis quand n√©cessaire, et tu avances l'histoire sc√®ne par sc√®ne.",
                        campaign_id,
                    )
                    # Reprendre le dernier user prompt d'intro depuis le state si possible
                    try:
                        intro_msg = next((m["content"] for m in st.session_state.history if m["role"] == "user"), None)
                    except Exception:
                        intro_msg = None
                    if intro_msg:
                        store_message_optimized(user_id, "user", intro_msg, campaign_id)
            except Exception:
                pass
        else:
            # Afficher et stocker le message utilisateur saisi
            with st.chat_message("user"):
                st.markdown(prompt)
            st.session_state.history.append({"role": "user", "content": prompt})
            store_message_optimized(user_id, "user", prompt, campaign_id)

        # G√©n√©rer la r√©ponse avec gestion d'erreurs am√©lior√©e
        with st.spinner("üé≤ Le Ma√Ætre du Jeu r√©fl√©chit..."):
            reply = None
            error_occurred = False
            retry_count = 0
            max_retries = 2

            while retry_count <= max_retries and reply is None:
                try:
                    start_time = time.time()
                    ai_response = call_ai_model_optimized(model, st.session_state.history)
                    latency = time.time() - start_time

                    reply = ai_response["content"]

                    # Stocker les performances seulement si succ√®s
                    store_performance_optimized(
                        user_id, model, latency, ai_response["tokens_in"], ai_response["tokens_out"], campaign_id
                    )

                    # Afficher des m√©triques en temps r√©el
                    cost = calculate_estimated_cost(model, ai_response["tokens_in"], ai_response["tokens_out"])
                    st.caption(f"‚ö° {latency:.2f}s | üé´ {ai_response['tokens_out']} tokens | üí∞ ${cost:.4f}")
                    break

                except ChatbotError as e:
                    error_message = str(e)

                    # Gestion sp√©cifique des erreurs de quota OpenAI
                    if (
                        ("quota" in error_message.lower() and "d√©pass√©" in error_message.lower())
                        or ("billing" in error_message.lower() and "limit" in error_message.lower())
                        or ("insufficient_quota" in error_message.lower())
                    ):
                        import os

                        from src.ai.models_config import get_available_alternative_models

                        available_alternatives = get_available_alternative_models(model)

                        # Option exp√©rimentale : basculement automatique
                        auto_fallback = os.getenv("AI_AUTO_FALLBACK", "false").lower() == "true"

                        if auto_fallback and available_alternatives:
                            # Essayer automatiquement avec le premier mod√®le alternatif disponible
                            fallback_model = available_alternatives[0]
                            logger.info(f"Basculement automatique de {model} vers {fallback_model} (quota √©puis√©)")

                            try:
                                # R√©essayer avec le mod√®le alternatif
                                ai_response = call_ai_model_optimized(fallback_model, st.session_state.history)
                                latency = time.time() - start_time

                                reply = (
                                    f"üîÑ **Basculement automatique** : {model} ‚Üí {fallback_model}\n\n{ai_response['content']}"
                                )

                                # Stocker les performances avec le nouveau mod√®le
                                store_performance_optimized(
                                    user_id,
                                    fallback_model,
                                    latency,
                                    ai_response["tokens_in"],
                                    ai_response["tokens_out"],
                                    campaign_id,
                                )

                                # Afficher des m√©triques
                                cost = calculate_estimated_cost(
                                    fallback_model, ai_response["tokens_in"], ai_response["tokens_out"]
                                )
                                st.caption(
                                    f"‚ö° {latency:.2f}s | üé´ {ai_response['tokens_out']} tokens | üí∞ ${cost:.4f} | üîÑ Mod√®le: {fallback_model}"
                                )

                                # Succ√®s avec le mod√®le alternatif
                                break

                            except Exception as fallback_error:
                                logger.warning(f"√âchec du basculement automatique vers {fallback_model}: {fallback_error}")
                                # Continuer avec le message d'erreur normal

                        # Message d'erreur normal avec suggestions
                        alt_text = ""
                        if available_alternatives:
                            alt_text = f"\n\nüîÑ **Mod√®les alternatifs disponibles :**\n"
                            for alt_model in available_alternatives[:3]:  # Limite √† 3 suggestions
                                alt_config = get_model_config(alt_model)
                                cost_comparison = alt_config.cost_per_1k_input
                                alt_text += f"‚Ä¢ **{alt_model}** - ${cost_comparison:.4f}/1K tokens ({alt_config.description[:40]}...)\n"
                            alt_text += f"\n‚ú® **Suggestion :** Changez de mod√®le dans les param√®tres ou via le s√©lecteur en haut de page."
                            if not auto_fallback:
                                alt_text += f"\n\nüîß **Basculement automatique** : Ajoutez `AI_AUTO_FALLBACK=true` dans votre .env pour un basculement automatique."
                        else:
                            alt_text = f"\n\n‚ö†Ô∏è **Aucun mod√®le alternatif configur√©.** Ajoutez des cl√©s API pour Anthropic ou DeepSeek dans votre fichier .env."

                        reply = (
                            f"‚ùå **Quota OpenAI √©puis√©** ‚õΩ\n\n"
                            f"Votre limite de facturation OpenAI a √©t√© atteinte.\n\n"
                            f"üîß **Solutions possibles :**\n"
                            f"‚Ä¢ V√©rifiez votre compte OpenAI et augmentez votre limite\n"
                            f"‚Ä¢ Attendez le renouvellement de votre quota mensuel{alt_text}\n\n"
                            f"üí° Votre conversation est sauvegard√©e et vous pourrez continuer plus tard."
                        )
                        logger.error(f"Quota OpenAI √©puis√©: {e}")
                        error_occurred = True
                        break
                    elif "rate limit" in error_message.lower() or "429" in error_message:
                        # Rate limit - retry possible
                        if retry_count < max_retries:
                            st.warning(
                                f"‚è≥ Limite de d√©bit OpenAI atteinte, nouvel essai dans quelques secondes... (tentative {retry_count + 1}/{max_retries + 1})"
                            )
                            time.sleep(2**retry_count)  # Backoff exponentiel
                            retry_count += 1
                            continue
                        else:
                            reply = f"‚ùå **Rate Limit OpenAI :** Trop de requ√™tes cons√©cutives.\n\nüí° **Solution :** Attendez quelques minutes ou essayez un autre mod√®le dans les param√®tres."
                    else:
                        # Autres erreurs techniques
                        reply = f"‚ùå **Erreur technique :** {error_message}\n\nüîÑ **Vous pouvez :** R√©essayer votre derni√®re action ou reformuler votre message."
                    logger.error(f"Erreur ChatbotError: {e}")
                    error_occurred = True
                    break

                except Exception as e:
                    if retry_count < max_retries:
                        st.warning(f"‚ö†Ô∏è Erreur de connexion, nouvel essai... (tentative {retry_count + 1}/{max_retries + 1})")
                        time.sleep(1)
                        retry_count += 1
                        continue
                    else:
                        reply = f"‚ùå **Erreur de connexion :** Impossible de contacter le serveur AI.\n\nüîÑ **Suggestions :**\n- V√©rifiez votre connexion internet\n- R√©essayez dans quelques instants\n- La conversation reste sauvegard√©e"
                        logger.error(f"Erreur inattendue dans chat apr√®s {max_retries} tentatives: {e}")
                        error_occurred = True
                        break

        # Afficher la r√©ponse et sauvegarder
        response_container = st.chat_message("assistant")
        with response_container:
            st.markdown(reply)

            # Bouton de retry si erreur
            if error_occurred and not auto_trigger:
                if st.button(f"üîÑ R√©essayer la derni√®re action", key=f"retry_{len(st.session_state.history)}"):
                    # Ne pas ajouter la r√©ponse d'erreur √† l'historique
                    st.rerun()

        # Sauvegarder seulement si pas d'erreur ou si c'est une erreur informative
        if reply and not error_occurred:
            st.session_state.history.append({"role": "assistant", "content": reply})
            store_message_optimized(user_id, "assistant", reply, campaign_id)
        elif reply and error_occurred:
            # Pour les erreurs, ajouter un message syst√®me informatif mais pas la r√©ponse d'erreur compl√®te
            error_summary = "Erreur AI - voir message pr√©c√©dent pour d√©tails"
            st.session_state.history.append({"role": "assistant", "content": error_summary})
            store_message_optimized(user_id, "assistant", error_summary, campaign_id)

        # Auto-scroll vers le nouveau message + forcer un rerun
        st.markdown(
            """
            <script>
            // Auto-scroll vers le bas apr√®s nouveau message
            setTimeout(function() {
                window.scrollTo(0, document.body.scrollHeight);
                // Alternative pour Streamlit
                const chatContainer = document.querySelector('[data-testid="stChatMessage"]');
                if (chatContainer) {
                    chatContainer.scrollIntoView({ behavior: 'smooth', block: 'end' });
                }
            }, 100);
            </script>
            """,
            unsafe_allow_html=True,
        )

        # Forcer un rerun pour r√©-afficher l'historique AU-DESSUS du champ d'entr√©e
        try:
            st.rerun()
        except Exception:
            pass


# ========================================
# ALIASES POUR R√âTROCOMPATIBILIT√â TESTS
# ========================================


def store_message(user_id: int, role: str, content: str, campaign_id: Optional[int] = None) -> None:
    """Alias pour r√©trocompatibilit√© avec les tests."""
    return store_message_optimized(user_id, role, content, campaign_id)


def store_performance(
    user_id: int, model: str, latency: float, tokens_in: int, tokens_out: int, campaign_id: Optional[int] = None
) -> None:
    """Alias pour r√©trocompatibilit√© avec les tests."""
    return store_performance_optimized(user_id, model, latency, tokens_in, tokens_out, campaign_id)


def launch_chat_interface(user_id: int) -> None:
    """Alias pour r√©trocompatibilit√© avec les tests."""
    return launch_chat_interface_optimized(user_id)


def call_ai_model(model: str, messages: List[Dict], temperature: float = 0.8) -> Dict[str, Any]:
    """Alias pour r√©trocompatibilit√© avec les tests."""
    return call_ai_model_optimized(model, messages, temperature)


# Fonctions manquantes pour les tests (impl√©mentations simples)
def get_last_model(user_id: int) -> Optional[str]:
    """R√©cup√®re le dernier mod√®le utilis√© par l'utilisateur."""
    try:
        from src.data.models import get_user_model_choice

        return get_user_model_choice(user_id)
    except:
        return "GPT-4"


def get_previous_history(user_id: int, campaign_id: Optional[int] = None, limit: int = 50) -> List[Dict]:
    """R√©cup√®re l'historique des messages pr√©c√©dents."""
    try:
        from src.data.models import get_campaign_messages

        return get_campaign_messages(user_id, campaign_id, limit)
    except:
        return []
