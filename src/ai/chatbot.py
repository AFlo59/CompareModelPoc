"""
Chatbot optimis√© avec gestion d'erreurs et performance am√©lior√©es
"""

import logging
import time
from typing import Any, Dict, List, Optional

import streamlit as st

from src.ai.api_client import get_anthropic_client, get_deepseek_client, get_openai_client
from src.ai.models_config import CHAT_DEFAULTS, ModelProvider, calculate_estimated_cost, get_model_config
from src.data.database import get_connection

logger = logging.getLogger(__name__)


class ChatbotError(Exception):
    """Exception personnalis√©e pour les erreurs de chatbot."""

    pass


class APIManager:
    """Gestionnaire optimis√© des appels API avec retry et timeout."""

    @staticmethod
    def call_openai_model(model_config, messages: List[Dict], temperature: float = None) -> Dict[str, Any]:
        """Appelle un mod√®le OpenAI avec gestion d'erreurs."""
        client = get_openai_client()
        temperature = temperature or model_config.temperature_default

        try:
            response = client.chat.completions.create(
                model=model_config.api_name,
                messages=messages,
                temperature=temperature,
                max_tokens=model_config.max_tokens,
                timeout=CHAT_DEFAULTS["timeout"],
            )

            return {
                "content": response.choices[0].message.content,
                "tokens_in": response.usage.prompt_tokens,
                "tokens_out": response.usage.completion_tokens,
                "model": model_config.name,
            }
        except Exception as e:
            logger.error(f"Erreur OpenAI pour {model_config.name}: {e}")
            raise ChatbotError(f"Erreur OpenAI: {str(e)}")

    @staticmethod
    def call_anthropic_model(model_config, messages: List[Dict], temperature: float = None) -> Dict[str, Any]:
        """Appelle un mod√®le Anthropic avec gestion d'erreurs."""
        client = get_anthropic_client()
        temperature = temperature or model_config.temperature_default

        try:
            # S√©parer le message syst√®me des autres messages
            system_msg = ""
            user_messages = []

            for msg in messages:
                if msg["role"] == "system":
                    system_msg = msg["content"]
                else:
                    user_messages.append(msg)

            # S'assurer qu'il y a au moins un message utilisateur
            if not user_messages:
                user_messages = [{"role": "user", "content": "Commen√ßons l'aventure !"}]

            response = client.messages.create(
                model=model_config.api_name,
                max_tokens=model_config.max_tokens,
                temperature=temperature,
                system=system_msg if system_msg else "Tu es un assistant IA.",
                messages=user_messages,
                timeout=CHAT_DEFAULTS["timeout"],
            )

            return {
                "content": response.content[0].text,
                "tokens_in": response.usage.input_tokens,
                "tokens_out": response.usage.output_tokens,
                "model": model_config.name,
            }
        except Exception as e:
            logger.error(f"Erreur Anthropic pour {model_config.name}: {e}")
            raise ChatbotError(f"Erreur Anthropic: {str(e)}")

    @staticmethod
    def call_deepseek_model(model_config, messages: List[Dict], temperature: float = None) -> Dict[str, Any]:
        """Appelle un mod√®le DeepSeek (API compatible OpenAI)."""
        client = get_deepseek_client()
        if client is None:
            raise ChatbotError("Cl√© API DeepSeek manquante")

        temperature = temperature or model_config.temperature_default

        try:
            response = client.chat.completions.create(
                model=model_config.api_name,
                messages=messages,
                temperature=temperature,
                max_tokens=model_config.max_tokens,
                timeout=CHAT_DEFAULTS["timeout"],
            )

            return {
                "content": response.choices[0].message.content,
                "tokens_in": response.usage.prompt_tokens,
                "tokens_out": response.usage.completion_tokens,
                "model": model_config.name,
            }
        except Exception as e:
            logger.error(f"Erreur DeepSeek pour {model_config.name}: {e}")
            raise ChatbotError(f"Erreur DeepSeek: {str(e)}")


def call_ai_model_optimized(model_name: str, messages: List[Dict], temperature: float = None) -> Dict[str, Any]:
    """
    Appelle le mod√®le d'IA appropri√© avec gestion optimis√©e.

    Args:
        model_name: Nom du mod√®le √† utiliser
        messages: Liste des messages de conversation
        temperature: Temp√©rature pour la g√©n√©ration (optionnel)

    Returns:
        Dict contenant 'content', 'tokens_in', 'tokens_out', 'model'

    Raises:
        ChatbotError: En cas d'erreur dans l'appel API
    """
    model_config = get_model_config(model_name)

    try:
        if model_config.provider == ModelProvider.OPENAI.value:
            return APIManager.call_openai_model(model_config, messages, temperature)
        elif model_config.provider == ModelProvider.ANTHROPIC.value:
            return APIManager.call_anthropic_model(model_config, messages, temperature)
        elif model_config.provider == ModelProvider.DEEPSEEK.value:
            return APIManager.call_deepseek_model(model_config, messages, temperature)
        else:
            # Fallback vers GPT-4 pour les mod√®les non support√©s
            logger.warning(f"Mod√®le {model_name} non support√©, fallback vers GPT-4")
            fallback_config = get_model_config("GPT-4")
            return APIManager.call_openai_model(fallback_config, messages, temperature)

    except ChatbotError:
        raise  # Re-raise les erreurs de chatbot
    except Exception as e:
        logger.error(f"Erreur inattendue avec le mod√®le {model_name}: {e}")
        raise ChatbotError(f"Erreur inattendue: {str(e)}")


def store_message_optimized(user_id: int, role: str, content: str, campaign_id: Optional[int] = None) -> None:
    """Stocke un message avec gestion d'erreurs optimis√©e."""
    try:
        with get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute(
                "INSERT INTO messages (user_id, role, content, campaign_id) VALUES (?, ?, ?, ?)",
                (user_id, role, content, campaign_id),
            )
            conn.commit()
    except Exception as e:
        logger.error(f"Erreur stockage message: {e}")
        # Ne pas propager l'erreur pour ne pas casser l'exp√©rience utilisateur
        st.warning("Message non sauvegard√© (erreur technique)")


def store_performance_optimized(
    user_id: int, model: str, latency: float, tokens_in: int, tokens_out: int, campaign_id: Optional[int] = None
) -> None:
    """Stocke les donn√©es de performance avec calcul de co√ªt."""
    try:
        estimated_cost = calculate_estimated_cost(model, tokens_in, tokens_out)

        with get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute(
                """INSERT INTO performance_logs 
                   (user_id, model, latency, tokens_in, tokens_out, campaign_id) 
                   VALUES (?, ?, ?, ?, ?, ?)""",
                (user_id, model, latency, tokens_in, tokens_out, campaign_id),
            )
            conn.commit()

        logger.info(f"Performance enregistr√©e: {model}, {latency:.2f}s, co√ªt estim√©: ${estimated_cost:.4f}")
    except Exception as e:
        logger.error(f"Erreur stockage performance: {e}")
        # Ne pas propager l'erreur


def launch_chat_interface_optimized(user_id: int) -> None:
    """Interface de chat optimis√©e avec gestion d'erreurs am√©lior√©e."""

    # V√©rification de la campagne
    if "campaign" not in st.session_state or not st.session_state.campaign:
        st.error("‚ùå Aucune campagne s√©lectionn√©e")
        if st.button("üè∞ Choisir une campagne"):
            st.session_state.page = "campaign_or_resume"
            st.rerun()
        return

    campaign = st.session_state.campaign
    campaign_id = campaign.get("id")

    # D√©terminer le mod√®le automatiquement (campagne -> pr√©f√©rence utilisateur -> d√©faut)
    try:
        from src.data.models import get_user_model_choice

        user_pref = get_user_model_choice(user_id)
    except Exception:
        user_pref = None

    model = campaign.get("ai_model") or user_pref or "GPT-4"
    model_config = get_model_config(model)
    # Afficher une m√©trique informative plut√¥t qu'un s√©lecteur
    info_col1, info_col2 = st.columns([3, 1])
    with info_col1:
        st.caption(f"ü§ñ Mod√®le actif: {model}")
    with info_col2:
        st.metric("üí∞ Co√ªt/1K tokens", f"${model_config.cost_per_1k_input:.3f}")

    # Initialisation de l'historique
    if "history" not in st.session_state:
        st.session_state.history = []

    # D√©marrage automatique si demand√© (auto_start_intro)
    auto_trigger = False
    try:
        auto_trigger = bool(getattr(st.session_state, "auto_start_intro", False) or st.session_state.get("auto_start_intro"))
    except Exception:
        auto_trigger = False

    # Affichage de l'historique AVANT, champ de saisie APR√àS
    for msg in st.session_state.history:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    # Champ de saisie (en bas)
    prompt = st.chat_input("Votre action ?")
    user_submitted = bool(prompt)

    if auto_trigger or user_submitted:
        if auto_trigger:
            # Retirer le flag pour ne pas boucler
            try:
                st.session_state.auto_start_intro = False
            except Exception:
                try:
                    st.session_state["auto_start_intro"] = False
                except Exception:
                    pass
            # Si l'historique initial n'a pas √©t√© persist√© (changement de page rapide), persister en t√¢che de fond
            try:
                from src.data.models import get_campaign_messages

                messages = get_campaign_messages(user_id, campaign_id, 2)
                if not messages:
                    # Reconstituer les deux premiers messages si absents
                    store_message_optimized(
                        user_id,
                        "system",
                        "Tu es un MJ immersif, concis quand n√©cessaire, et tu avances l'histoire sc√®ne par sc√®ne.",
                        campaign_id,
                    )
                    # Reprendre le dernier user prompt d'intro depuis le state si possible
                    try:
                        intro_msg = next((m["content"] for m in st.session_state.history if m["role"] == "user"), None)
                    except Exception:
                        intro_msg = None
                    if intro_msg:
                        store_message_optimized(user_id, "user", intro_msg, campaign_id)
            except Exception:
                pass
        else:
            # Afficher et stocker le message utilisateur saisi
            with st.chat_message("user"):
                st.markdown(prompt)
            st.session_state.history.append({"role": "user", "content": prompt})
            store_message_optimized(user_id, "user", prompt, campaign_id)

        # G√©n√©rer la r√©ponse
        with st.spinner("üé≤ Le Ma√Ætre du Jeu r√©fl√©chit..."):
            try:
                start_time = time.time()
                ai_response = call_ai_model_optimized(model, st.session_state.history)
                latency = time.time() - start_time

                reply = ai_response["content"]

                # Stocker les performances
                store_performance_optimized(
                    user_id, model, latency, ai_response["tokens_in"], ai_response["tokens_out"], campaign_id
                )

                # Afficher des m√©triques en temps r√©el
                cost = calculate_estimated_cost(model, ai_response["tokens_in"], ai_response["tokens_out"])
                st.caption(f"‚ö° {latency:.2f}s | üé´ {ai_response['tokens_out']} tokens | üí∞ ${cost:.4f}")

            except ChatbotError as e:
                reply = f"‚ùå **Erreur technique :** {str(e)}\n\nVeuillez r√©essayer ou changer de mod√®le."
                logger.error(f"Erreur ChatbotError: {e}")
            except Exception as e:
                reply = f"‚ùå **Erreur inattendue :** Une erreur s'est produite.\n\nVeuillez r√©essayer."
                logger.error(f"Erreur inattendue dans chat: {e}")

        # Afficher la r√©ponse et sauvegarder
        with st.chat_message("assistant"):
            st.markdown(reply)

        st.session_state.history.append({"role": "assistant", "content": reply})
        store_message_optimized(user_id, "assistant", reply, campaign_id)

        # Forcer un rerun pour r√©-afficher l'historique AU-DESSUS du champ d'entr√©e
        # (sinon, la premi√®re r√©ponse peut appara√Ætre sous la zone input lors du d√©clenchement auto)
        try:
            st.rerun()
        except Exception:
            pass


# ========================================
# ALIASES POUR R√âTROCOMPATIBILIT√â TESTS
# ========================================


def store_message(user_id: int, role: str, content: str, campaign_id: Optional[int] = None) -> None:
    """Alias pour r√©trocompatibilit√© avec les tests."""
    return store_message_optimized(user_id, role, content, campaign_id)


def store_performance(
    user_id: int, model: str, latency: float, tokens_in: int, tokens_out: int, campaign_id: Optional[int] = None
) -> None:
    """Alias pour r√©trocompatibilit√© avec les tests."""
    return store_performance_optimized(user_id, model, latency, tokens_in, tokens_out, campaign_id)


def launch_chat_interface(user_id: int) -> None:
    """Alias pour r√©trocompatibilit√© avec les tests."""
    return launch_chat_interface_optimized(user_id)


def call_ai_model(model: str, messages: List[Dict], temperature: float = 0.8) -> Dict[str, Any]:
    """Alias pour r√©trocompatibilit√© avec les tests."""
    return call_ai_model_optimized(model, messages, temperature)


# Fonctions manquantes pour les tests (impl√©mentations simples)
def get_last_model(user_id: int) -> Optional[str]:
    """R√©cup√®re le dernier mod√®le utilis√© par l'utilisateur."""
    try:
        from src.data.models import get_user_model_choice

        return get_user_model_choice(user_id)
    except:
        return "GPT-4"


def get_previous_history(user_id: int, campaign_id: Optional[int] = None, limit: int = 50) -> List[Dict]:
    """R√©cup√®re l'historique des messages pr√©c√©dents."""
    try:
        from src.data.models import get_campaign_messages

        return get_campaign_messages(user_id, campaign_id, limit)
    except:
        return []
